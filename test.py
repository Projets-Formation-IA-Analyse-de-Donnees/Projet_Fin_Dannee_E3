import os
import time
import numpy as np
import hdbscan
from collections import Counter
from qdrant_client import QdrantClient
from sklearn.preprocessing import normalize
from sklearn.metrics import silhouette_score
import umap.umap_ as umap # Assurez-vous d'avoir installÃ© 'umap-learn'

# -------------------------------
# CONFIG QDRANT
# -------------------------------
QDRANT_HOST = os.getenv("QDRANT_HOST", "localhost")
QDRANT_PORT = int(os.getenv("QDRANT_PORT", "6333"))
COLLECTION_NAME = "articles" # Assurez-vous que c'est bien la collection avec les bons embeddings !

client = QdrantClient(host=QDRANT_HOST, port=QDRANT_PORT)


# -------------------------------
# 1. RÃ©cupÃ©ration et nettoyage des embeddings (INCHANGÃ‰)
# -------------------------------
def get_embeddings_from_qdrant(filter_code=None):
    """RÃ©cupÃ¨re tous les vecteurs valides de Qdrant."""
    vectors, ids, payloads = [], [], []
    total_points, invalid_vectors = 0, 0

    print("ğŸš€ DÃ©marrage de la rÃ©cupÃ©ration des vecteurs depuis Qdrant...")
    
    # Utilisation de scroll pour gÃ©rer de grands volumes de donnÃ©es
    try:
        scroll_result = client.scroll(
            collection_name=COLLECTION_NAME,
            with_payload=True,
            with_vectors=True,
            limit=1000
        )
        
        # Le rÃ©sultat est maintenant une tuple (points, next_page_offset)
        current_points = scroll_result[0]
        next_page = scroll_result[1]

        while current_points:
            for point in current_points:
                total_points += 1
                vec = point.vector
                
                # Appliquer le filtre si spÃ©cifiÃ©
                if filter_code and point.payload.get("code_parent") != filter_code:
                    continue

                # Validation robuste du vecteur
                if (
                    isinstance(vec, list)
                    and len(vec) > 0 # VÃ©rifier que la liste n'est pas vide
                    and not any(v is None for v in vec)
                    and not any(np.isnan(v) for v in vec)
                ):
                    vectors.append(vec)
                    ids.append(point.id)
                    payloads.append(point.payload)
                else:
                    invalid_vectors += 1

            if not next_page:
                break
            
            scroll_result = client.scroll(
                collection_name=COLLECTION_NAME,
                with_payload=True,
                with_vectors=True,
                limit=1000,
                offset=next_page
            )
            current_points = scroll_result[0]
            next_page = scroll_result[1]
    
    except Exception as e:
        print(f"âŒ Erreur lors de la communication avec Qdrant : {e}")
        return np.array([]), [], []


    print(f"âœ… {len(vectors)} vecteurs valides sur {total_points} points rÃ©cupÃ©rÃ©s.")
    if invalid_vectors:
        print(f"âš ï¸ {invalid_vectors} vecteurs invalides ont Ã©tÃ© filtrÃ©s.")
        
    return np.array(vectors), ids, payloads


# -------------------------------
# 2. Pipeline de Clustering : UMAP + HDBSCAN (NOUVEAU)
# -------------------------------
def run_clustering_pipeline(X_original_normalized, umap_params, hdbscan_params):
    """
    ExÃ©cute le pipeline complet : UMAP pour la rÃ©duction, puis HDBSCAN pour le clustering.
    Retourne les labels et le score silhouette.
    """
    
    # --- Ã‰tape A : RÃ©duction de dimensionnalitÃ© avec UMAP ---
    print(f"ğŸ”„ [UMAP] RÃ©duction vers {umap_params['n_components']} dimensions...")
    
    reducer = umap.UMAP(
        n_neighbors=umap_params['n_neighbors'],
        n_components=umap_params['n_components'],
        min_dist=umap_params['min_dist'],
        metric='cosine',  # La meilleure mÃ©trique pour les embeddings sÃ©mantiques
        random_state=42   # Garantit la reproductibilitÃ© des rÃ©sultats
    )
    X_reduced = reducer.fit_transform(X_original_normalized)
    
    # --- Ã‰tape B : Clustering avec HDBSCAN sur les donnÃ©es rÃ©duites ---
    print(f"ğŸ”„ [HDBSCAN] DÃ©tection des clusters...")
    
    clusterer = hdbscan.HDBSCAN(
        min_cluster_size=hdbscan_params['min_cluster_size'],
        min_samples=hdbscan_params.get('min_samples'), # min_samples est optionnel
        metric=hdbscan_params['metric'], # 'euclidean' est efficace sur les donnÃ©es rÃ©duites par UMAP
        cluster_selection_epsilon=hdbscan_params.get('cluster_selection_epsilon', 0.0)
    )
    labels = clusterer.fit_predict(X_reduced)

    # --- Ã‰tape C : Ã‰valuation ---
    n_clusters = len(set(labels)) - (1 if -1 in labels else 0)
    n_noise = list(labels).count(-1)
    
    silhouette = -1 # Score par dÃ©faut si le clustering Ã©choue
    if n_clusters > 1:
        # Le score silhouette est le plus pertinent sur les donnÃ©es que le clusterer a vues
        silhouette = silhouette_score(X_reduced, labels)

    cluster_sizes = Counter(labels[labels != -1])
    avg_size = np.mean(list(cluster_sizes.values())) if cluster_sizes else 0

    print("\n--- RÃ©sultats pour cette configuration ---")
    print(f"ğŸ“Š Clusters trouvÃ©s : {n_clusters}")
    print(f"ğŸ”‡ Articles isolÃ©s (bruit) : {n_noise} ({n_noise / len(labels):.1%})")
    print(f"ğŸ“ Taille moyenne des clusters : {avg_size:.2f}")
    print(f"ğŸ† Score Silhouette (sur donnÃ©es rÃ©duites) : {silhouette:.3f}")
    
    return labels, silhouette


# -------------------------------
# 3. Benchmark et Utilisation (MIS Ã€ JOUR)
# -------------------------------
if __name__ == "__main__":
    
    # --- Ã‰tape 1 : Chargement et prÃ©paration des donnÃ©es ---
    # On ne le fait qu'une seule fois pour tout le benchmark.
    vectors, ids, payloads = get_embeddings_from_qdrant(filter_code="LEGITEXT000006071307")
    
    if vectors.shape[0] < 10:
        print("âŒ Pas assez de vecteurs pour lancer le clustering. ArrÃªt du script.")
    else:
        # La normalisation est cruciale avant UMAP avec la mÃ©trique 'cosine'
        print("\nğŸ“ Normalisation des vecteurs originaux...")
        vectors_normalized = normalize(vectors)

        # --- Ã‰tape 2 : DÃ©finition des configurations Ã  tester ---
        # Chaque configuration est un dictionnaire avec des paramÃ¨tres pour UMAP et HDBSCAN.
        configs_to_test = [
            # Config Ã©quilibrÃ©e, bon point de dÃ©part
            {
                "umap": {"n_neighbors": 15, "n_components": 10, "min_dist": 0.0},
                "hdbscan": {"min_cluster_size": 10, "min_samples": 3, "metric": "euclidean"}
            },
            # Vise des clusters plus petits et plus nombreux
            {
                "umap": {"n_neighbors": 10, "n_components": 5, "min_dist": 0.0},
                "hdbscan": {"min_cluster_size": 5, "min_samples": 1, "metric": "euclidean"}
            },
            # Vise des clusters plus larges et une vision plus globale
            {
                "umap": {"n_neighbors": 50, "n_components": 15, "min_dist": 0.1},
                "hdbscan": {"min_cluster_size": 15, "min_samples": 5, "metric": "euclidean"}
            },
            # Autre variation pour des clusters trÃ¨s denses
             {
                "umap": {"n_neighbors": 20, "n_components": 8, "min_dist": 0.0},
                "hdbscan": {"min_cluster_size": 12, "min_samples": None, "metric": "euclidean"}
            },
        ]
        
        best_score = -1
        best_config = None
        best_labels = None
        
        # --- Ã‰tape 3 : Lancement du benchmark ---
        start_time_benchmark = time.time()
        for i, cfg in enumerate(configs_to_test):
            print(f"\n============================== CONFIGURATION {i+1}/{len(configs_to_test)} ==============================")
            print(f"UMAP params: {cfg['umap']}")
            print(f"HDBSCAN params: {cfg['hdbscan']}")
            
            labels, silhouette = run_clustering_pipeline(
                vectors_normalized, 
                umap_params=cfg["umap"], 
                hdbscan_params=cfg["hdbscan"]
            )
            
            # On cherche la configuration qui maximise le score Silhouette
            if silhouette > best_score:
                best_score = silhouette
                best_config = cfg
                best_labels = labels

        elapsed_benchmark = time.time() - start_time_benchmark
        print(f"\n\n============================== FIN DU BENCHMARK ({elapsed_benchmark:.2f}s) ==============================")
        print("âœ¨ RÃ©sultat final âœ¨")
        
        if best_config:
            print(f"ğŸ† Meilleure configuration trouvÃ©e :")
            print(f"   - UMAP: {best_config['umap']}")
            print(f"   - HDBSCAN: {best_config['hdbscan']}")
            print(f"   - Score Silhouette : {best_score:.4f}")

            # --- Ã‰tape 4 : Analyse optionnelle des meilleurs clusters ---
            if best_labels is not None:
                print("\nğŸ” Analyse des clusters de la meilleure configuration :")
                cluster_sizes = Counter(best_labels)
                # Afficher les 5 plus grands clusters (en excluant le bruit -1)
                if -1 in cluster_sizes:
                    del cluster_sizes[-1]
                
                print(f"   -> {len(cluster_sizes)} clusters trouvÃ©s.")
                for cluster_id, size in cluster_sizes.most_common(5):
                    print(f"\n   --- Cluster #{cluster_id} (Taille: {size}) ---")
                    # Retrouver les titres des 5 premiers articles de ce cluster
                    indices_in_cluster = [i for i, label in enumerate(best_labels) if label == cluster_id]
                    for j in range(min(5, len(indices_in_cluster))):
                        article_index = indices_in_cluster[j]
                        article_payload = payloads[article_index]
                        print(f"     - Article: {article_payload.get('title', 'N/A')} (ID: {ids[article_index]})")
        else:
            print("Aucune configuration n'a produit de rÃ©sultat valide.")